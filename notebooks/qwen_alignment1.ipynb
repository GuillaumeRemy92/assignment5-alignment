{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ad1717",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bd05929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-10 18:33:14 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from vllm import LLM, SamplingParams\n",
    "from sympy import sympify, simplify \n",
    "from typing import Callable, List, Dict\n",
    "from pathlib import Path\n",
    "from cs336_alignment.drgrpo_grader import r1_zero_reward_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a65b757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device setup (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "515cc65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory for serialized results\n",
    "OUTPUT_DIR = Path(\"zero_shot_results\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69e776",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0b3428",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e499d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "R1_ZERO_PROMPT = \"\"\"\n",
    "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\n",
    "User: {question}\n",
    "Assistant: <think>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38519837",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ca413d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load the Dataset\n",
    "# Load GSM8K test split (1339 examples)\n",
    "dataset = load_dataset(\"gsm8k\", \"main\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4c2c4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1319 examples from GSM8K test set.\n"
     ]
    }
   ],
   "source": [
    "# Prepare list of prompts and ground truths\n",
    "prompts = []\n",
    "ground_truths = []\n",
    "for example in dataset:\n",
    "    question = example[\"question\"].strip()\n",
    "    prompt = R1_ZERO_PROMPT.format(question=question)\n",
    "    prompts.append(prompt)\n",
    "    # GSM8K ground truth is in \"answer\" field, e.g., \"Natalia sold clips... ### 72\"\n",
    "    # Extract the final answer (after \"###\")\n",
    "    gt = example[\"answer\"].split(\"###\")[-1].strip()\n",
    "    ground_truths.append(gt)\n",
    "\n",
    "print(f\"Loaded {len(prompts)} examples from GSM8K test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b69dd4",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01b42d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-10 18:33:16 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 12-10 18:33:24 config.py:542] This model supports multiple tasks: {'classify', 'score', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 12-10 18:33:24 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='Qwen/Qwen2.5-Math-1.5B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-Math-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 12-10 18:33:25 cuda.py:179] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 12-10 18:33:25 cuda.py:227] Using XFormers backend.\n",
      "INFO 12-10 18:33:26 model_runner.py:1110] Starting to load model Qwen/Qwen2.5-Math-1.5B...\n",
      "INFO 12-10 18:33:26 weight_utils.py:252] Using model weights format ['*.safetensors']\n",
      "INFO 12-10 18:33:26 weight_utils.py:297] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3997dc1a90d455a824402bf3398ac56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-10 18:33:28 model_runner.py:1115] Loading model weights took 2.8797 GB\n",
      "INFO 12-10 18:33:30 worker.py:267] Memory profiling takes 0.94 seconds\n",
      "INFO 12-10 18:33:30 worker.py:267] the current vLLM instance can use total_gpu_memory (14.56GiB) x gpu_memory_utilization (0.90) = 13.11GiB\n",
      "INFO 12-10 18:33:30 worker.py:267] model weights take 2.88GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 8.78GiB.\n",
      "INFO 12-10 18:33:30 executor_base.py:110] # CUDA blocks: 20557, # CPU blocks: 9362\n",
      "INFO 12-10 18:33:30 executor_base.py:115] Maximum concurrency for 4096 tokens per request: 80.30x\n",
      "INFO 12-10 18:33:33 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:18<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-10 18:33:52 model_runner.py:1562] Graph capturing finished in 18 secs, took 0.17 GiB\n",
      "INFO 12-10 18:33:52 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 23.52 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load the Model with vLLM\n",
    "# Load Qwen2.5-Math-1.5B with vLLM (downloads automatically if not cached)\n",
    "model_path = \"Qwen/Qwen2.5-Math-1.5B\"\n",
    "vllm_model = LLM(\n",
    "    model=model_path,\n",
    "    dtype=\"float16\",  # As recommended in PDF for memory efficiency\n",
    "    gpu_memory_utilization=0.9,  # Adjust if OOM errors\n",
    "    tensor_parallel_size=1,  # Single GPU\n",
    ")\n",
    "\n",
    "# Sampling params (from PDF: temp=1.0, top_p=1.0, max=1024, stop on </answer>)\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=1024,\n",
    "    stop=[\"</answer>\"],\n",
    "    include_stop_str_in_output=True  # To include the stop token in output if generated\n",
    ")\n",
    "\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f330131",
   "metadata": {},
   "source": [
    "# Zero-shot benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdb11661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Evaluation Function\n",
    "def evaluate_vllm(\n",
    "    vllm_model: LLM,\n",
    "    reward_fn: Callable[[str, str], Dict[str, float]],\n",
    "    prompts: List[str],\n",
    "    ground_truths: List[str],\n",
    "    eval_sampling_params: SamplingParams,\n",
    "    output_path: str = \"results.json\"\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Evaluate model on prompts, compute metrics, serialize to disk.\n",
    "    Returns list of dicts with example, generation, scores.\n",
    "    \"\"\"\n",
    "    outputs = vllm_model.generate(prompts, eval_sampling_params)\n",
    "    \n",
    "    results = []\n",
    "    for idx, output in enumerate(outputs):\n",
    "        generated_text = output.outputs[0].text.strip()  # vLLM generated continuation\n",
    "        full_response = output.prompt + generated_text  # Full for analysis (prompt + generation)\n",
    "        \n",
    "        # Pass full_response or generated_text to reward_fn – test which one works with the official fn\n",
    "        # From PDF, parsing the \"model’s output\" likely means the generation after <think>\n",
    "        # Try generated_text first; if not, switch to full_response\n",
    "        rewards = reward_fn(generated_text, ground_truths[idx])  # <-- Updated to use official fn; adjust arg if needed\n",
    "        \n",
    "        result = {\n",
    "            \"prompt\": prompts[idx],\n",
    "            \"generation\": generated_text,\n",
    "            \"full_response\": full_response,\n",
    "            \"ground_truth\": ground_truths[idx],\n",
    "            \"format_reward\": rewards[\"format_reward\"],\n",
    "            \"answer_reward\": rewards[\"answer_reward\"]\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    # Serialize to disk\n",
    "    with open(OUTPUT_DIR / output_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    print(f\"Results saved to {OUTPUT_DIR / output_path}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63b94ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ground truths contain a '# ' before every number which causes the r1_zero_reward_fn function to fail, we remove it here. \n",
    "ground_truths_v2 = [x[2:] for x in ground_truths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5435bb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1319/1319 [02:43<00:00,  8.09it/s, est. speed input: 1253.69 toks/s, output: 2232.35 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to zero_shot_results/gsm8k_zero_shot.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Run Evaluation\n",
    "# Run evaluation on full dataset (or slice for testing: prompts[:100])\n",
    "results = evaluate_vllm(\n",
    "    vllm_model=vllm_model,\n",
    "    reward_fn=r1_zero_reward_fn,  # <-- Now using the official function\n",
    "    prompts=prompts,\n",
    "    ground_truths=ground_truths_v2,\n",
    "    eval_sampling_params=sampling_params,\n",
    "    output_path=\"gsm8k_zero_shot.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fbcea0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Counts:\n",
      "{'correct_both': 6, 'format_ok_answer_wrong': 22, 'both_wrong': 1291}\n",
      "\n",
      "Overall Accuracy (correct both): 0.45%\n",
      "Format Success Rate: 2.12%\n",
      "\n",
      "Examples where both correct:\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: The bagel cost $4.\n",
      "  The soup cost 25% more than the bagel, so the soup cost $4 x 1.25 = $5.\n",
      "  The cake cost half of the price of the bagel, so the cake cost $4 / 2 = $2.\n",
      "  The total cost of the dinner is $4 (bagel) + $5 (soup) + $2 (cake) = $11.\n",
      "</think> <answer> $11 </answer>\n",
      "Ground Truth: 11\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: The starting value is 20. Let's call the starting value \\( S \\). So, \\( S = 20 \\).\n",
      "\n",
      "The calculation is:\n",
      "\\[ S + \\frac{S}{2} \\div 5 \\]\n",
      "First, calculate half of the starting value:\n",
      "\\[ \\frac{S}{2} = \\frac{20}{2} = 10 \\]\n",
      "Now, add this to the starting value:\n",
      "\\[ S + 10 = 20 + 10 = 30 \\]\n",
      "Next, divide by 5:\n",
      "\\[ 30 \\div 5 = 6 \\]\n",
      "Finally, multiply by the starting value minus 12:\n",
      "\\[ 6 \\times (S - 12) = 6 \\times (20 - 12) = 6 \\times 8 = 48 \\]\n",
      "\n",
      "So, the final value of the number train is \\(\\boxed{48}\\).\n",
      "</think> <answer>48</answer>\n",
      "Ground Truth: 48\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: The length of one of Topher’s shoes is 8-feet and 4-inches. Let's convert 8 feet to inches to add with the additional 4 inches. Since 1 foot is equal to 12 inches, 8 feet is equal to \\( 8 \\times 12 = 96 \\) inches. Adding the additional 4 inches, the length of one of Topher’s shoes is \\( 96 + 4 = 100 \\) inches.\n",
      "\n",
      "Next, we need to set up the equation for the length of one of Bobby’s shoes. Let the length of one of Bobby’s shoes be \\( x \\) inches. According to the problem, the length of one of Topher’s shoes is 10 inches longer than 9 times the length of one of Bobby’s shoes. This can be written as:\n",
      "\n",
      "\\[ 100 = 9x + 10 \\]\n",
      "\n",
      "To find \\( x \\), we need to solve this equation:\n",
      "\n",
      "\\[ 100 - 10 = 9x \\]\n",
      "\\[ 90 = 9x \\]\n",
      "\\[ x = \\frac{90}{9} \\]\n",
      "\\[ x = 10 \\]\n",
      "\n",
      "So, the length of one of Bobby’s shoes is 10 inches.\n",
      "</think> <answer>10</answer>\n",
      "Ground Truth: 10\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: - 4 pizzas and a total cost of 64 dollars\n",
      "- 2 pizzas cost 30 dollars\n",
      "- 1 pizza costs 34 dollars\n",
      "- Both pizzas cost 17 dollars each\n",
      "</think> <answer>\n",
      "17\n",
      "</answer>\n",
      "Ground Truth: 17\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: <newline>\n",
      "  To find out how old Vince is, we first need to determine Liam's age two years ago. Since Liam is currently 16 years old, two years ago he was 16 - 2 = 14 years old. At that time, his age was twice the age of Vince, so we can write the equation:\n",
      "  L = 2(V-2)\n",
      "  where L is Liam's age two years ago and V is Vince's current age.\n",
      "  <newline>\n",
      "  <equation>\n",
      "    L = 2V - 4\n",
      "    14 = 2V - 4\n",
      "    2V = 18\n",
      "    V = 9\n",
      "  </equation>\n",
      "</think> <answer>9</answer>\n",
      "Ground Truth: 9\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: To find out how long it would take John to speak 10 pages, we first need to calculate his new speaking speed after training. His new speed is 2.5 times his starting speed, which is 150 WPM. So, his new speaking speed is 2.5 * 150 = 375 WPM.\n",
      "\n",
      "Next, we need to calculate the total number of words he needs to speak to complete 10 pages. Each page has 450 words, so for 10 pages, he needs to speak 10 * 450 = 4500 words.\n",
      "\n",
      "Finally, to find out how long it would take him to speak 4500 words at a speed of 375 WPM, we divide the total number of words by his new speaking speed: 4500 / 375 = 12 minutes.\n",
      "</think> <answer>12 minutes</answer>\n",
      "Ground Truth: 12\n",
      "---\n",
      "\n",
      "Examples where format OK but answer wrong:\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: Let's say they lost x games. Then they won x+8 games.\n",
      "Now, x+(x+8)=22\n",
      "Combine like terms. We'll get 2x+8=22\n",
      "Now, we'll isolate the variable x by moving 8 to the right side.\n",
      "2x+8-8=22-8\n",
      "x=7\n",
      "Now, we'll plug in the number to find out how many games won.\n",
      "7+(7+8)=(14+8)=22\n",
      "The number of games won is 15 and the number of games lost is 7.\n",
      "</think> <answer>Now, we'll plug in the number to find out how many games won.</answer>\n",
      "Ground Truth: 15\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: log<sub>4</sub>$x + 8*log<sub>4</sub>$x^(1/5) = 6$, solve for real x.\n",
      "</think> <answer>\n",
      "Assuming this is a fixed point iteration question, the mp = [f'(-1514.45), 1], so c = -1 514.45.\n",
      "</answer>\n",
      "Ground Truth: 114,200\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: Let's denote the number of candies Robert has as R. According to the problem, James has 6 more candies than Robert, so James has R + 6 candies. John has twice as many candies as Robert, and we know that John has 54 candies, so R = 54 / 2 = 27 candies. James has 27 + 6 = 33 candies. Therefore, John has 54 - 33 = 21 more candies than James.\n",
      "</think> <answer> John has 21 more candies than James. </answer>\n",
      "Ground Truth: 21\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: Line 1: Sean has to memorize the lines from his solo song (54 lines).\n",
      "Line 2: The first scene has twice the number of lines, so it has 2 * 54 = 108 lines. But only a third of them are his lines, so he has to memorize 108 / 3 = 36 lines for the first scene.\n",
      "Line 3: The second scene has six more lines than the song, so it has 54 + 6 = 60 lines. Four-fifths of them are his lines, so he has to memorize 60 * (4/5) = 48 lines for the second scene.\n",
      "Line 4: To find out how many lines Sean has to memorize, we add up the lines from all three scenes: 54 + 36 + 48 = 138 lines.\n",
      "Conclusion: Sean has to memorize 138 lines.\n",
      "Answer: 138 lines.\n",
      "</think> <answer>138 lines</answer>\n",
      "Ground Truth: 138\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: Let's denote the distance traveled by both cars as D, the speed of the car in the fast lane as S1, and the speed of the car in the slow lane as S2. We know that S1 = 60 mi/h and S2 = S1/2 = 30 mi/h.\n",
      "\n",
      "The time taken to travel a certain distance is equal to the ratio of the distance to the speed. Therefore, the time taken by the car in the fast lane to travel 480 miles is T1 = D/S1.\n",
      "\n",
      "The time taken by the car in the slow lane to travel the same distance is T2 = D/S2.\n",
      "\n",
      "Since S2 = S1/2, we can say that T2 = 2T1.\n",
      "\n",
      "Substituting the values, we get T2 = 2 * (480/60) = 16 hours.\n",
      "\n",
      "Answer:\n",
      "The time taken by the car in the slow lane to cover the same distance is 16 hours.\n",
      "</think> <answer> Time taken by the car in the slow lane = 16 hours </answer>\n",
      "Ground Truth: 16\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: Quinn is 30, so Jane is 5 fewer. So she is 30 - 5 = 25. Trent is 5 years older than Jane, so he is 25 + 5 = 30.\n",
      "</think> <answer>Trent is 25 years old.</answer>\n",
      "Ground Truth: 32\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: • Customer spent $1 off for every $20 spent\n",
      "• Customer spent $80 last shopping trip\n",
      "• Customer spent $43 on this shopping trip\n",
      "• They applied a coupon for twice the amount the rewards\n",
      "• Compensation off the price cured by some\n",
      "• Amount直接影响 during: reward - purchase discount\n",
      "• Amount of purchase during:\n",
      "• Reward = 1/2 dollars per purchase every 10 purchase\n",
      "• First purchase - $80 customer spent\n",
      "• Second purchase - $43 customer spent\n",
      "• Response: customer paid (precisely) $61\n",
      "</think> <answer> $61 </answer>\n",
      "Ground Truth: 31\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: Tue – 1c\n",
      "Thu – 2c\n",
      "Sat – 2(c)=4c\n",
      "Total – ?\n",
      "Answer\n",
      "Totals = 1c+2c+4c\n",
      "Totals = 7c\n",
      "Totals = 7 hours  \n",
      "</think> <answer>7 hours</answer>\n",
      "Ground Truth: 5\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: The weight of the 1959 penny is given as 48 grains. The weight of the 2010 penny is three-quarters of the weight of the 1959 penny.\n",
      "So, the weight of the 2010 penny is (3/4) * 48 = 36 grains.\n",
      "The combined weight of the two pennies is 48 + 36 = 84 grains.\n",
      "</think> <answer> 84 grains</answer>\n",
      "Ground Truth: 84\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: 1. First, we need to find out how many eggs can fit on 2 trays together.\n",
      "  2. Each tray holds 24 eggs, so 2 trays hold 24 * 2 = 48 eggs.\n",
      "  3. User has 64 eggs and can only fit 48 eggs on the trays.\n",
      "  4. So, the number of eggs User won't be able to place on the tray is 64 - 48 = 16 eggs.\n",
      "</think> <answer> 16 eggs </answer>\n",
      "Ground Truth: 16\n",
      "---\n",
      "\n",
      "Examples where both wrong (format failed):\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: cost of 16 ducks per day = 16*$1\n",
      "cost of 3 eggs for breakfast = 3*$1\n",
      "cost of 4 muffins = 4*$0\n",
      "money earned in a day from selling 3 eggs per day= sold price*quantity = 3*2\n",
      "money earned in a day from selling 4 muffins per day= sold price*quantity = 4*0\n",
      "money earned in a day from selling 16 - (3+4) ducks = (16 - (3+4))*2 = 11*2\n",
      "money earned in a day from selling 16-3-4 ducks net (after expenses) = past total money - (3*2+4*0)\n",
      "Total money = 16*1 + 11*2 + (3*2+4*0) = 49\n",
      "Ankit [121]\n",
      "<img>/cimages/multimages/16/capture210463508685166922503.jpg</img>\n",
      "Ground Truth: 18\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: Take 2 blue fibers\n",
      "Take half of blue fiber: 2 / 2 = 1\n",
      "Total = 2 + 1 = 3\n",
      "</think>\n",
      "<answer> 3 </answer>\n",
      "Ground Truth: 3\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: $50,000 +$80,000 = $130,000 because it is asking how much did Josh spend total on the house\n",
      "$130,000 x 150% = $195,000 this is the new value of the house\n",
      "$195,000 - $130,000 = $65,000 this is where his profit is.\n",
      "</think>\n",
      "<answer> The profit is $65,000.\n",
      "</answer>\n",
      "Ground Truth: 70000\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: User decides to run 3 sprints 3 times a week.  He runs 60 meters each sprint.  How many\n",
      "total meters does he run a week?\n",
      "Scene: User writes down the calculation on a piece of paper. He uses a notepad editor installed in his mobile device, which also sets a calculation compendium.\n",
      "60 x 3 = 180 meters\n",
      "180 meters x 3 = 540 meters\n",
      "540 meters / 7 days = approximately 77 meters a day\n",
      "</think>\n",
      "Assistant's response: He runs a total of 540 meters a week.\n",
      "<answer>\n",
      "User: James runs a total of 540 meters a week.\n",
      "Ground Truth: 540\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: 1. The total amount of feed Wendi gives her chickens in the morning is 15 cups.\n",
      "  2. The total amount of feed Wendi gives her chickens in the afternoon is 25 cups.\n",
      "  3. The total amount of feed Wendi needs to give her 20 chickens in a day is 3 cups per chicken per day.\n",
      "  4. Therefore, the total amount of feed Wendi needs to give her 20 chickens in a day is 20 chickens * 3 cups per chicken per day = 60 cups.\n",
      "  5. The amount of feed Wendi needs to give her 20 chickens in the final meal of the day is 60 cups - 15 cups - 25 cups = 20 cups.\n",
      "</think>\n",
      "<answer> 20 cups </answer>\n",
      "Ground Truth: 20\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: To solve this problem, we need to calculate the total cost of buying 16 glasses, where every second glass costs 60% of the regular price. The regular price of a glass is $5. So, the cost of every second glass is $5 * 0.60 = $3.00.\n",
      "\n",
      "Since Kylar is buying 16 glasses, there are 16 / 2 = 8 glasses that are priced at $3.00 each, and 8 glasses that are priced at $5.00 each. The total cost will be the sum of the cost of the first set of glasses and the cost of the second set of glasses (which correspond to every second glass).\n",
      "\n",
      "Let's calculate this step by step.\n",
      "```python\n",
      "# Define the cost of a regular glass\n",
      "regular_price = 5.00\n",
      "\n",
      "# Define the discounted price for every second glass\n",
      "discounted_price = regular_price * 0.60\n",
      "\n",
      "# Total number of glasses Kylar wants to buy\n",
      "total_glasses = 16\n",
      "\n",
      "# Calculate the number of regular-priced glasses and discounted-priced glasses\n",
      "num_regular_glasses = total_glasses // 2\n",
      "num_discounted_glasses = total_glasses // 2\n",
      "\n",
      "# Calculate the total cost\n",
      "total_cost = (num_regular_glasses * regular_price) + (num_discounted_glasses * discounted_price)\n",
      "print(total_cost)\n",
      "```\n",
      "```output\n",
      "68.0\n",
      "```\n",
      "The total amount Kylar needs to pay for 16 glasses is $\\boxed{68.00}$.\n",
      "Ground Truth: 64\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: ...\n",
      "...\n",
      "...\n",
      "</think>\n",
      "...\n",
      "<answer>\n",
      "...\n",
      "</answer>\n",
      "Ground Truth: 260\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: Here is one way to look at the problem.\n",
      "\n",
      "Rewrite in terms of fractions, so we have a fraction of the fraction\n",
      "\n",
      "$.\\frac{2}{5} \\cdot 200 \\mathrm{GB}= \\frac{2}{5} \\cdot \\frac{200}{1} \\mathrm{GB}$\n",
      "\n",
      "<repeat> $\\frac{2}{5} = 2 \\text{ over } \\ 5 \\\\$\n",
      "\n",
      "<thunk> slice the top into 5 equal pieces, and take 2 of them <endl>\n",
      "</repeat>\n",
      "</think>\n",
      "\n",
      "$2 \\cdot 40 =80 \\text{ GB downloaded}$\n",
      "</repeat>\n",
      "</thunk>\n",
      "\n",
      "Think the way it works after the first restart ...\n",
      "\n",
      "$.\\frac{2}{5} \\cdot 200 \\mathrm{GB}= \\frac{2}{5} \\cdot \\frac{200}{1} \\mathrm{GB}$\n",
      "\n",
      "<repeat> $\\frac{2}{5} = 2 \\text{ over } \\ 5 \\\\$\n",
      "\n",
      "<thunk> slice the top into 5 equal pieces, and take 2 of them <endl>\n",
      "</repeat>\n",
      "</thunk>\n",
      "</repeat>\n",
      "</thunk>\n",
      "\n",
      "So we have two total restarts, and 20 total minutes of glue: 2*200GB=200GB+200GB+200GB+200GB=800GB. Also 80+20+20=120 minutes of glue.\n",
      "\n",
      "So $800 GB \\over 120 min = {200 Over x}$\n",
      "\n",
      "1. the ${200GB} shipment takes 3\\frac{1}{3}$ hours mindlessly computing $\\frac{12\\frac{2}{3}}{10}\\cdot20=240/3=80$.\n",
      "\n",
      "Assistant is thinking\n",
      "$$\\frac{2}{5} \\cdot \\frac{200}{1} = \\frac{2}{5} \\cdot 200=80 \\\\ \\mbox{(60 min)}.$$\n",
      "\n",
      "Super AM has-Star Forum-Bot\n",
      "All work we belong, from + mine.\n",
      "---------------------------------------------------------------\n",
      "\n",
      "A few things are worth learning...\n",
      "\n",
      "The <thunk>, <repeat>, <thunk>, <repeat> they're the same thing. This works.\n",
      "Induction works.\n",
      "\n",
      "Assistant is thinking\n",
      "$$80 \\mbox{ (60 min)}$$\n",
      "\n",
      "Assistant is thinking\n",
      "\\begin{alignat*}{4}\n",
      "&&\\mathrm{I\\!~} && \\quad 80 \\sum^{2}_{i=1}2^i \\\\\n",
      "\\mathrm{I\\!~} &&= && \\quad 160 \\\\\n",
      "\\mathrm{I\\!~} \\sum^2_{i=1}2\\mathrm{(}2^{i-1}\\mathrm{\\!~)} &= && \\quad 4 + 8 \\\\\n",
      "\\mathrm{I\\!~} \\sum^2_{i=1}2\\mathrm{(}2^{i-1}\\mathrm{\\!~)} =\\exp && \\quad \\sum^1_{i=1}\\mathrm{(}2\\mathrm{(}2^{i-1}\\mathrm{\\!~)}\\backslash \\\\ &&&&&&&& \\mathrm{I\\!~} \\sum^2_{i=1}2\\mathrm{(}2^{i-1}\\mathrm{\\!~)} &= && \\quad 12 \\\\\n",
      "\\end{alignat*}\n",
      "\n",
      "A few things are worth learning...\n",
      "\n",
      "The <thunk>, <repeat>, <thunk>, <repeat> they're the same thing. This works.\n",
      "Induction works.\n",
      "\n",
      "Someone has a monster on poor metal no hair.\n",
      "Ground Truth: 160\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: Consider Conservation of Momentum in a 1-D problem\n",
      "Momentum Initial = Momentum Final\n",
      "m1v1i + m2v2i = m1v1f + m2v2f\n",
      "Here , v1f = 0\n",
      "From this equation we can calculate m1 and m2 as \n",
      "m1 = (v2i(t2 + t3)) / vi2\n",
      "and\n",
      "m2 = (v2i(t2 + t3)/(m1 + vi1))...\n",
      "Solving above we get, m = 30 mph\n",
      "Now t3= 3/4 hour\n",
      "t4= 2/4 hour\n",
      "So that's why m = 60 x 3/4 = 45 miles.\n",
      "t4= 1/4 hour\n",
      "t5= 2/4 Hour\n",
      "m5= 80mph x 2/4 = 40\n",
      "t6 = 3/4 HOUR\n",
      "t7= 1/4 hour\n",
      "Now we get the displacement of spaceship from home place, Delta x = 60 x t1 + (1/4) ((45x 60 + 15)) and \n",
      "t2 = 5/12 hour\n",
      "t3+ t6 = 1/3 hour\n",
      "Thus from 60 mph hour we have covered 5 miles.\n",
      "Thus, overall Delta x = 60x(3 + 2 + t3+t4+ Dt=5) + (1/4) ((45x 60 + 15)) = 315 + 75 = 390 miles\n",
      "The User is 390 miles away from home in 4 hours.\n",
      "The User is thinking that the problem can be solved through Equations of Kinematics, and then asks the correct way of solving through equations of Kinematics.\n",
      "Ground Truth: 45\n",
      "---\n",
      "Prompt: \n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. T...\n",
      "Generation: - Eliza's regular rate per hour is $10.\n",
      "  - Eliza's overtime rate per hour is $10 \\* 1.2 = $12.\n",
      "  - Eliza has worked 40 \\* $10 + 5 \\* $12 = $400 + $60 = $460 for the week.\n",
      "</think>\n",
      "answer\n",
      "$460\n",
      "</answer>\n",
      "Ground Truth: 460\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Categorize generations\n",
    "category_counts = {\n",
    "    \"correct_both\": 0,  # format=1, answer=1\n",
    "    \"format_ok_answer_wrong\": 0,  # format=1, answer=0\n",
    "    \"both_wrong\": 0  # format=0, answer=0\n",
    "}\n",
    "\n",
    "examples = {\n",
    "    \"correct_both\": [],\n",
    "    \"format_ok_answer_wrong\": [],\n",
    "    \"both_wrong\": []\n",
    "}\n",
    "\n",
    "for res in results:\n",
    "    fr = res[\"format_reward\"]\n",
    "    ar = res[\"answer_reward\"]\n",
    "    if fr == 1 and ar == 1:\n",
    "        category_counts[\"correct_both\"] += 1\n",
    "        if len(examples[\"correct_both\"]) < 10:\n",
    "            examples[\"correct_both\"].append(res)\n",
    "    elif fr == 1 and ar == 0:\n",
    "        category_counts[\"format_ok_answer_wrong\"] += 1\n",
    "        if len(examples[\"format_ok_answer_wrong\"]) < 10:\n",
    "            examples[\"format_ok_answer_wrong\"].append(res)\n",
    "    else:\n",
    "        category_counts[\"both_wrong\"] += 1\n",
    "        if len(examples[\"both_wrong\"]) < 10:\n",
    "            examples[\"both_wrong\"].append(res)\n",
    "\n",
    "print(\"Category Counts:\")\n",
    "print(category_counts)\n",
    "\n",
    "# Overall metrics\n",
    "total = len(results)\n",
    "accuracy = (category_counts[\"correct_both\"] / total) * 100 if total > 0 else 0\n",
    "format_rate = ((category_counts[\"correct_both\"] + category_counts[\"format_ok_answer_wrong\"]) / total) * 100\n",
    "print(f\"\\nOverall Accuracy (correct both): {accuracy:.2f}%\")\n",
    "print(f\"Format Success Rate: {format_rate:.2f}%\")\n",
    "\n",
    "# For writeup: Inspect examples (observe at least 10 per category if available)\n",
    "print(\"\\nExamples where both correct:\")\n",
    "for ex in examples[\"correct_both\"]:\n",
    "    print(f\"Prompt: {ex['prompt'][:100]}...\")\n",
    "    print(f\"Generation: {ex['generation']}\")\n",
    "    print(f\"Ground Truth: {ex['ground_truth']}\")\n",
    "    print(\"---\")\n",
    "\n",
    "print(\"\\nExamples where format OK but answer wrong:\")\n",
    "for ex in examples[\"format_ok_answer_wrong\"]:\n",
    "    print(f\"Prompt: {ex['prompt'][:100]}...\")\n",
    "    print(f\"Generation: {ex['generation']}\")\n",
    "    print(f\"Ground Truth: {ex['ground_truth']}\")\n",
    "    print(\"---\")\n",
    "\n",
    "print(\"\\nExamples where both wrong (format failed):\")\n",
    "for ex in examples[\"both_wrong\"]:\n",
    "    print(f\"Prompt: {ex['prompt'][:100]}...\")\n",
    "    print(f\"Generation: {ex['generation']}\")\n",
    "    print(f\"Ground Truth: {ex['ground_truth']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4aa175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f095d953",
   "metadata": {},
   "source": [
    "# Supervised fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da8092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-Math-1.5B\"\n",
    "OUTPUT_DIR = Path(\"sft_checkpoints\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98dbe185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc8ad66bae84603a75f6a9e267f0a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 93733 high-quality math SFT examples\n",
      "Features: {'problem': Value('string'), 'solution': Value('string'), 'answer': Value('string'), 'problem_type': Value('string'), 'question_type': Value('string'), 'source': Value('string'), 'uuid': Value('string'), 'is_reasoning_complete': List(Value('bool')), 'generations': List(Value('string')), 'correctness_math_verify': List(Value('bool')), 'correctness_llama': List(Value('bool')), 'finish_reasons': List(Value('string')), 'correctness_count': Value('int64'), 'messages': List({'content': Value('string'), 'role': Value('string')})}\n",
      "\n",
      "First example:\n",
      "{\n",
      "  \"problem\": \"## Task B-1.3.\\n\\nA ship traveling along a river has covered $24 \\\\mathrm{~km}$ upstream and $28 \\\\mathrm{~km}$ downstream. For this journey, it took half an hour less than for traveling $30 \\\\mathrm{~km}$ upstream and $21 \\\\mathrm{~km}$ downstream, or half an hour more than for traveling $15 \\\\mathrm{~km}$ upstream and $42 \\\\mathrm{~km}$ downstream, assuming that both the ship and the river move uniformly.\\n\\nDetermine the speed of the ship in still water and the speed of the river.\",\n",
      "  \"solution\": \"## Solution.\\n\\nLet $t$ be the time required for the boat to travel $24 \\\\mathrm{~km}$ upstream and $28 \\\\mathrm{~km}$ downstream, $v_{R}$ the speed of the river, and $v_{B}$ the speed of the boat. When the boat is traveling upstream, its speed is $v_{B}-v_{R}$, and when it is traveling downstream, its speed is $v_{B}+v_{R}$.\\n\\nSince $t=\\\\frac{s}{v}$, from the given data, we obtain the following system of equations:\\n\\n$\\\\left\\\\{\\\\begin{array}{l}t=\\\\frac{24}{v_{B}-v_{R}}+\\...\n"
     ]
    }
   ],
   "source": [
    "# Load the high-quality math SFT dataset (220k verified traces from DeepSeek-R1)\n",
    "sft_dataset = load_dataset(\"open-r1/OpenR1-Math-220k\", split=\"train\")\n",
    "\n",
    "print(f\"Loaded {len(sft_dataset)} high-quality math SFT examples\")\n",
    "print(\"Features:\", sft_dataset.features)\n",
    "\n",
    "# Inspect first example\n",
    "print(\"\\nFirst example:\")\n",
    "print(json.dumps(sft_dataset[0], indent=2)[:1000] + \"...\" if len(str(sft_dataset[0])) > 1000 else json.dumps(sft_dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c77adea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully formatted 8000 examples!\n",
      "\n",
      "--- Sample Formatted Example ---\n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\n",
      "\n",
      "User: ## Task B-1.3.\n",
      "\n",
      "A ship traveling along a river has covered $24 \\mathrm{~km}$ upstream and $28 \\mathrm{~km}$ downstream. For this journey, it took half an hour less than for traveling $30 \\mathrm{~km}$ upstream and $21 \\mathrm{~km}$ downstream, or half an hour more than for traveling $15 \\mathrm{~km}$ upstream and $42 \\mathrm{~km}$ downstream, assuming that both the ship and the river move uniformly.\n",
      "\n",
      "Determine the speed of the ship in still water and the speed of the river.\n",
      "Assistant: <think>## Solution.\n",
      "\n",
      "Let $t$ be the time required for the boat to travel $24 \\mathrm{~km}$ upstream and $28 \\mathrm{~km}$ downstream, $v_{R}$ the speed of the river, and $v_{B}$ the speed of the boat. When the boat is traveling upstream, its speed is $v_{B}-v_{R}$, and when it is traveling downstream, its speed is $v_{B}+v_{R}$.\n",
      "\n",
      "Since $t=\\frac{s}{v}$, from the given data, we obtain the following system of equations:\n",
      "\n",
      "$\\left\\{\\begin{array}{l}t=\\frac{24}{v_{B}-v_{R}}+\\frac{28}{v_{B}+v_{R}} \\\\ t+0.5=\\frac{30}{v_{B}-v_{R}}+\\frac{21}{v_{B}+v_{R}} \\\\ t-0.5=\\frac{15}{v_{B}-v_{R}}+\\frac{42\n",
      "...\n",
      "ollowing the same procedure, the initial system transforms into the system $\\left\\{\\begin{array}{l}6 x-7 y=0.5 \\\\ 9 x-14 y=0.5\\end{array}\\right.$\n",
      "\n",
      "The solution to this system is $\\left(\\frac{1}{6}, \\frac{1}{14}\\right)$.</think><answer>v_{R}=4\\mathrm{~}/\\mathrm{},v_{B}=10\\mathrm{~}/\\mathrm{}</answer>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# You already loaded it — reuse the variable\n",
    "# sft_dataset = load_dataset(\"open-r1/OpenR1-Math-220k\", split=\"train\")\n",
    "\n",
    "R1_ZERO_SYSTEM = \"\"\"A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\"\"\"\n",
    "\n",
    "def format_example(ex):\n",
    "    problem = ex[\"problem\"].strip()\n",
    "    solution = ex[\"solution\"].strip()\n",
    "    answer = ex[\"answer\"].strip()\n",
    "    \n",
    "    # Clean up solution: remove final boxed answer if present (avoid duplication)\n",
    "    import re\n",
    "    cleaned_solution = re.sub(r\"\\\\boxed\\{.*?\\}\", \"\", solution, flags=re.DOTALL).strip()\n",
    "    \n",
    "    full_text = f\"{R1_ZERO_SYSTEM}\\n\\nUser: {problem}\\nAssistant: <think>{cleaned_solution}</think><answer>{answer}</answer>\"\n",
    "    return {\"text\": full_text}\n",
    "\n",
    "# Format 8000 examples (fast & sufficient)\n",
    "train_data = []\n",
    "for ex in sft_dataset.select(range(10000)):  # 10k → ~9k valid\n",
    "    formatted = format_example(ex)\n",
    "    train_data.append(formatted)\n",
    "    if len(train_data) >= 8000:\n",
    "        break\n",
    "\n",
    "print(f\"Successfully formatted {len(train_data)} examples!\")\n",
    "\n",
    "# Show one — should be PERFECT\n",
    "print(\"\\n--- Sample Formatted Example ---\")\n",
    "print(train_data[0][\"text\"][:1500] + \"\\n...\\n\" + train_data[0][\"text\"][-300:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7fed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046b379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datasets import Dataset   # ← This is the missing import!\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-Math-1.5B\"\n",
    "OUTPUT_DIR = Path(\"sft_final_r1_style\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Tokenizer (already loaded, but re-run if needed)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # ← This handles the 4-bit loading\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Model (REMOVED load_in_4bit=True from here — fixed!)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # load_in_4bit=True,  # ← DELETE THIS LINE — it's redundant!\n",
    "    quantization_config=bnb_config,  # ← This is enough\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # ~40M trainable params\n",
    "\n",
    "#train_dataset = Dataset.from_list(train_data)   # ← CONVERT LIST → HF DATASET\n",
    "train_dataset = Dataset.from_list(train_data[:64])\n",
    "\n",
    "print(f\"Converted to Dataset: {len(train_dataset)} examples\")\n",
    "print(\"Columns:\", train_dataset.column_names)\n",
    "print(\"First example keys:\", train_dataset[0].keys())\n",
    "\n",
    "# 2. Let SFTTrainer tokenize automatically (this is the easiest & safest way)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=1,           # ← Fits in 15GB\n",
    "        gradient_accumulation_steps=32,          # ← Effective batch = 32\n",
    "        learning_rate=2e-4,\n",
    "        num_train_epochs=1,\n",
    "        logging_steps=20,\n",
    "        save_strategy=\"epoch\",\n",
    "        bf16=True,\n",
    "        report_to=\"none\",\n",
    "        gradient_checkpointing=True,             # ← Huge memory saver\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},  # ← Required for Qwen\n",
    "        optim=\"paged_adamw_8bit\",                # ← Extra memory saving\n",
    "        fp16=False,\n",
    "    ),\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",        # ← SFTTrainer will tokenize this field automatically\n",
    "    max_seq_length=2048,\n",
    "    packing=False,                    # ← Keep False (packing + gradient_checkpointing can conflict)\n",
    ")\n",
    "\n",
    "print(\"Starting FINAL SFT — this WILL work and finish in ~60–90 min (or ~15 min if you use 2000 examples)\")\n",
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "print(\"SFT DONE! Your LoRA adapter is saved in:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b522d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "sft_model = LLM(\n",
    "    model=\"Qwen/Qwen2.5-Math-1.5B\",\n",
    "    dtype=\"bfloat16\",\n",
    "    load_format=\"bitsandbytes\",\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.9,\n",
    ")\n",
    "\n",
    "sft_model.load_lora_weights(\"sft_final_r1_style\")  # ← your trained adapter\n",
    "\n",
    "# Test on 10 random GSM8K problems\n",
    "test_prompts = prompts[::130]  # every 130th → 10 diverse examples\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.0,      # greedy for clean format\n",
    "    max_tokens=1024,\n",
    "    stop=[\"</answer>\"]\n",
    ")\n",
    "\n",
    "print(\"Generating from your SFT model...\\n\" + \"=\"*60)\n",
    "outputs = sft_model.generate(test_prompts, sampling_params)\n",
    "\n",
    "for i, out in enumerate(outputs):\n",
    "    gen = out.outputs[0].text\n",
    "    question = out.prompt.split(\"User:\")[-1].split(\"Assistant:\")[0].strip()[:120]\n",
    "    print(f\"\\nExample {i+1} | Question: {question}...\")\n",
    "    print(gen.strip())\n",
    "    print(\"Format correct?\", \"<think>\" in gen and \"</think>\" in gen and \"<answer>\" in gen and \"</answer>\" in gen)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d2a16b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a44333a",
   "metadata": {},
   "source": [
    "# Expert iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86528df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from cs336_alignment.drgrpo_grader import r1_zero_reward_fn\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# PATHS\n",
    "SFT_CHECKPOINT = \"sft_final_r1_style\"   # Your trained model\n",
    "EXPERT_DIR = Path(\"expert_iteration\")\n",
    "EXPERT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Loading your SFT model (merged checkpoint)...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(SFT_CHECKPOINT)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load your trained model directly (no PEFT, no bitsandbytes, no triton)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    SFT_CHECKPOINT,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=4,\n",
    ")\n",
    "\n",
    "# Use 100 GSM8K problems\n",
    "questions = [ex[\"question\"] for ex in dataset.select(range(100))]\n",
    "prompts = [R1_ZERO_PROMPT.format(question=q) for q in questions]\n",
    "\n",
    "print(\"Generating expert traces...\")\n",
    "expert_traces = []\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"Problem {i+1}/100\")\n",
    "    outputs = generator(prompt, num_return_sequences=4)\n",
    "    \n",
    "    gt = ground_truths[i]\n",
    "    for out in outputs:\n",
    "        gen = out['generated_text'][len(prompt):]\n",
    "        full = prompt + gen\n",
    "        \n",
    "        rewards = r1_zero_reward_fn(full, gt)\n",
    "        if rewards[\"format_reward\"] == 1.0 and rewards[\"answer_reward\"] == 1.0:\n",
    "            expert_traces.append({\"text\": full})\n",
    "            print(f\"  Found expert trace! Total: {len(expert_traces)}\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nFOUND {len(expert_traces)} EXPERT TRACES!\")\n",
    "\n",
    "if expert_traces:\n",
    "    expert_dataset = Dataset.from_list(expert_traces)\n",
    "    \n",
    "    # Train new model on expert traces\n",
    "    new_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"Qwen/Qwen2.5-Math-1.5B\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=new_model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=EXPERT_DIR / \"iter1\",\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=16,\n",
    "            learning_rate=2e-4,\n",
    "            num_train_epochs=1,\n",
    "            fp16=True,\n",
    "            gradient_checkpointing=True,\n",
    "            report_to=\"none\",\n",
    "        ),\n",
    "        train_dataset=expert_dataset,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=2048,\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model(EXPERT_DIR / \"iter1\")\n",
    "    print(\"EXPERT ITERATION COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47892241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alignment (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
